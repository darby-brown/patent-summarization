{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1D4o9Mpb79E4MmRL82uDG_sFz3cr76IQ5","timestamp":1702076670158},{"file_id":"1QKGg2ATRxGXMy2NkQqKV_FPrA5oNwXmo","timestamp":1701726138883},{"file_id":"1gN0qAHBCB0Ee-3hkGFAeplk_wA97YuEQ","timestamp":1700519377036}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Work 4: Extractive Summarization of Large Dataset\n","\n","\n","In this workbook, we work to mitigate the long length of the patents using extractive summarization (SumBasic) to capture the most important ideas (extraction on claims and description sections).\n","\n","Except there's not enough memory and it fails miserably every time. :D"],"metadata":{"id":"zrtZhQnsg5Ip"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"MiggKgtwJcSp"}},{"cell_type":"code","source":["#install libraries\n","!pip install -q datasets\n","!pip install -q sentencepiece\n","!pip install -q evaluate\n","!pip install -q rouge_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atabxkYjg7jF","outputId":"fe1512b4-be16-467d-8c47-65670c2a170b","executionInfo":{"status":"ok","timestamp":1702224511979,"user_tz":420,"elapsed":28202,"user":{"displayName":"Darby Brown","userId":"17895443117677718527"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["# Install Packages\n","\n","#standard data science libraries\n","import pandas as pd\n","import numpy as np\n","import random\n","import string\n","\n","#visualization\n","import matplotlib.pyplot as plt\n","from pprint import pprint\n","\n","#datasets\n","import datasets\n","from datasets import load_dataset, load_from_disk\n","\n","# PyTorch\n","# import torch\n","\n","#rouge\n","# import evaluate\n","\n","#NKLT for extractive summarization\n","import nltk\n","import nltk.corpus\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.tokenize import sent_tokenize, regexp_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist"],"metadata":{"id":"z7Hw8SIsljIY","executionInfo":{"status":"ok","timestamp":1702224515886,"user_tz":420,"elapsed":3908,"user":{"displayName":"Darby Brown","userId":"17895443117677718527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"30159cbf-59bc-49a9-fd84-b88842280472"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["# This cell will authenticate you and mount your Drive in the Colab.\n","##### ensure you mount to the folder that you want.\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","save_dir=\"/content/drive/MyDrive/W266/HUPD\""],"metadata":{"id":"BUgnXf9qJfv_","executionInfo":{"status":"ok","timestamp":1702224532509,"user_tz":420,"elapsed":16631,"user":{"displayName":"Darby Brown","userId":"17895443117677718527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"833ebcbd-0798-4477-9d19-b7c2c09be263"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##Load Data"],"metadata":{"id":"6p-CVzoVpwCv"}},{"cell_type":"code","source":["dataset_dict = load_from_disk(save_dir + '/HUPD_C07')"],"metadata":{"id":"ZagPe0AerhY4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Explore HUPD Dataset"],"metadata":{"id":"JixymefkpocS"}},{"cell_type":"code","source":["dataset_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TK63DuBVwBdl","executionInfo":{"status":"ok","timestamp":1702224619441,"user_tz":420,"elapsed":21,"user":{"displayName":"Darby Brown","userId":"17895443117677718527"}},"outputId":"68eb6c98-9f1d-42f2-e94e-73193cdbbf63"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n","        num_rows: 30915\n","    })\n","    validation: Dataset({\n","        features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n","        num_rows: 7311\n","    })\n","})"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#prepare training and validation sets\n","train_set = dataset_dict['train']\n","val_set = dataset_dict['validation']"],"metadata":{"id":"8pyb1r9Qpjzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#we must shrink the dataset for time concerns (google colab times out if any longer)\n","train_set = train_set.shuffle(seed=42).select(range(15000))\n","val_set = val_set.shuffle(seed=42).select(range(3750))"],"metadata":{"id":"Lr-VlG6pTMab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepare HUPD for Extractive Summarization -- We want to summarize the entire patent, not just one section."],"metadata":{"id":"Kiy5QYtdJsJ7"}},{"cell_type":"code","source":["train_set = train_set.map(lambda obs: {'claims_desc': obs['claims'] + obs['description']},\n","                          remove_columns=['claims','background','summary', 'description'])\n","val_set = val_set.map(lambda obs: {'claims_desc': obs['claims'] + obs['description']},\n","                          remove_columns=['claims','background','summary', 'description'])"],"metadata":{"id":"2ZjIt8O-JrjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extractive Summarization through SumBasic\n","\n","\n","\n","Here goes nothing."],"metadata":{"id":"aadDSNYJ25DB"}},{"cell_type":"code","source":["#score the sentences and print the highest scoring sentence with the highest scoring word\n","#keep repeating (with word score recalulation) until length is reached\n","\n","def sumbasic(lem_sentences, lem_words, len_summary = 45): #here we increase the number of sentences based on our findings from the first test\n","\n","    freq = FreqDist(lem_words)\n","    total = sum(freq.values())\n","    probs = {k: v/total for k, v in freq.items()}\n","\n","    summary = []\n","\n","    for _ in range(len_summary):\n","\n","        scores = {k: [] for k in lem_sentences}\n","        importance = {k: 0 for k in scores}\n","        for key, value in lem_sentences.items():               #recalulate the sentence scores\n","            for word in value:\n","                scores[key].append(probs[word])\n","            if len(scores[key]) <= 0:\n","              importance[key] = 0\n","            else:\n","              importance[key] = sum(scores[key]) / len(scores[key])\n","\n","        most_importance_sentence = max(scores, key=scores.get)  #pull out the most important sentence\n","        summary.append(most_importance_sentence)\n","\n","        for word in lem_sentences[most_importance_sentence]:    #recalculate word scores\n","            probs[word] = probs[word] * probs[word]\n","\n","    string_summary = ''\n","\n","    for sentence in lem_sentences:\n","        if sentence in summary:\n","            string_summary += sentence + ' '\n","    return string_summary\n"],"metadata":{"id":"6EnAZm_B2lvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extractive_summaries_train = []\n","\n","for obs in train_set['claims_desc']:\n","    #break obs first into sentences using NLTK's sent_tokenize\n","    all_sentences = sent_tokenize(obs)\n","\n","    #Let's walk through each of these sentences so we can divide into tokens (e.g. words)\n","    word_tokens = []\n","    sentence_tokens = {sentence: [] for sentence in all_sentences}\n","\n","    for one_sentence in all_sentences:\n","        for token in regexp_tokenize(one_sentence.lower(), '\\w+'):  #divide the sentences into tokens based on the regex for whitespace\n","            if token not in string.punctuation:\n","                            #ignore punctuation\n","                if token not in stopwords.words('english'):         #ignore stopwords\n","                    word_tokens.append(token)\n","                    sentence_tokens[one_sentence].append(token)\n","\n","    #A lemmatizer takes conjugated verbs and returns their infinitive form (e.g. conjugating -> conjugate)\n","    #It does the same thing with nouns taking the plural form and returning the singular form.\n","    #We're doing this because we want to count up occurences of word roots to get a tighter distribution\n","    lem = WordNetLemmatizer()\n","    lem_words = [lem.lemmatize(word) for word in word_tokens]\n","    lem_sentences = {sentence: [lem.lemmatize(word) for word in sentence_tokens[sentence]] for sentence in sentence_tokens}\n","\n","    #Now we have a list of lemmatized words and a list of sentences containing lemmatized words\n","    #we pass them to the sumbasic fiunction along with a size parameter\n","    #We'll also pass a summary size as a percentage of the sentences in the original document\n","    summary = sumbasic(lem_sentences, lem_words, len_summary = 30)\n","    extractive_summaries_train.append(summary)"],"metadata":{"id":"uzU_Rn623vqB","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1702244765577,"user_tz":420,"elapsed":20111919,"user":{"displayName":"Darby Brown","userId":"17895443117677718527"}},"outputId":"d29ab6c4-e59d-4d3d-9d60-de6202936d42"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-ba7e9e0f07c4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                             \u001b[0;31m#ignore punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m         \u001b[0;31m#ignore stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mword_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0msentence_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mone_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     19\u001b[0m         return [\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0mClose\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \"\"\"\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["len(extractive_summaries_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUg4sDfksfvS","executionInfo":{"status":"ok","timestamp":1702244814989,"user_tz":420,"elapsed":163,"user":{"displayName":"Darby Brown","userId":"17895443117677718527"}},"outputId":"9162002e-b46a-455c-e7b8-79f6e05a5f5b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6612"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["extracted_train_set = train_set.select(list(range(6612)))"],"metadata":{"id":"pCAYAaTktTUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extracted_train_set"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xf9HkcK8szYm","executionInfo":{"status":"ok","timestamp":1702245014472,"user_tz":420,"elapsed":411,"user":{"displayName":"Darby Brown","userId":"17895443117677718527"}},"outputId":"e22b9485-d92d-4e78-aea1-54115cfae53b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['patent_number', 'decision', 'title', 'abstract', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id', 'claims_desc'],\n","    num_rows: 6612\n","})"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#add extractive summaries to the train set\n","extracted_train_set = extracted_train_set.add_column('extractive_summaries', extractive_summaries_train)"],"metadata":{"id":"ZTiYgttSURVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#now we save the extracted data to Drive so that we can access it in our other Colab notebook\n","extracted_train_set.save_to_disk(save_dir + '/extracted_dataset_large_train')"],"metadata":{"id":"sqNrAXyVUZp2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extractive_summaries_val = []\n","\n","for obs in val_set['claims_desc']:\n","    #break obs first into sentences using NLTK's sent_tokenize\n","    all_sentences = sent_tokenize(obs)\n","\n","    #Let's walk through each of these sentences so we can divide into tokens (e.g. words)\n","    word_tokens = []\n","    sentence_tokens = {sentence: [] for sentence in all_sentences}\n","\n","    for one_sentence in all_sentences:\n","        for token in regexp_tokenize(one_sentence.lower(), '\\w+'):  #divide the sentences into tokens based on the regex for whitespace\n","            if token not in string.punctuation:                     #ignore punctuation\n","                if token not in stopwords.words('english'):         #ignore stopwords\n","                    word_tokens.append(token)\n","                    sentence_tokens[one_sentence].append(token)\n","\n","    #A lemmatizer takes conjugated verbs and returns their infinitive form (e.g. conjugating -> conjugate)\n","    #It does the same thing with nouns taking the plural form and returning the singular form.\n","    #We're doing this because we want to count up occurences of word roots to get a tighter distribution\n","    lem = WordNetLemmatizer()\n","    lem_words = [lem.lemmatize(word) for word in word_tokens]\n","    lem_sentences = {sentence: [lem.lemmatize(word) for word in sentence_tokens[sentence]] for sentence in sentence_tokens}\n","\n","    #Now we have a list of lemmatized words and a list of sentences containing lemmatized words\n","    #we pass them to the sumbasic fiunction along with a size parameter\n","    #We'll also pass a summary size as a percentage of the sentences in the original document\n","    summary = sumbasic(lem_sentences, lem_words, len_summary = 30)\n","    extractive_summaries_val.append(summary)"],"metadata":{"id":"ZF4aGqRHzTGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#add extractive summaries to the val set\n","val_set = val_set.add_column('extractive_summaries', extractive_summaries_val)"],"metadata":{"id":"m4IKKehDIngz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#now we save the extracted data to Drive so that we can access it in our other Colab notebook\n","val_set.save_to_disk(save_dir + '/extracted_dataset_large_val')"],"metadata":{"id":"c49LAGJDqV4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pprint(train_set['abstract'][3])"],"metadata":{"id":"YTii9H4hnT72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_set['extractive_summaries'][3].split(' '))"],"metadata":{"id":"97hLkro6n6VO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pprint(train_set['extractive_summaries'][3])"],"metadata":{"id":"fW2LV0nEoAvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DQvHbOWu2KC5"},"execution_count":null,"outputs":[]}]}